{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = os.path.realpath(os.path.join(os.path.dirname(__name__), '..', 'app','static','uploads', 'dataset.csv'))\n",
    "df_comments = pd.read_csv(dataset_path)\n",
    "df_comments.tail(n=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat jumlah data\n",
    "df_comments['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "combined_stopwords = set(stopword_factory.get_stop_words()).union(set(stopwords.words('english')))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text).lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in combined_stopwords]\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "\n",
    "df_comments['preprocess'] = df_comments['comment'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform TF data Test dengan data Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 23 stored elements and shape (2, 22)>\n",
      "  Coords\tValues\n",
      "  (0, 7)\t0.22346730896573733\n",
      "  (0, 11)\t0.44693461793147465\n",
      "  (0, 20)\t0.44693461793147465\n",
      "  (0, 4)\t0.22346730896573733\n",
      "  (0, 6)\t0.22346730896573733\n",
      "  (0, 0)\t0.3179976616659477\n",
      "  (0, 16)\t0.22346730896573733\n",
      "  (0, 2)\t0.22346730896573733\n",
      "  (0, 18)\t0.22346730896573733\n",
      "  (0, 5)\t0.22346730896573733\n",
      "  (0, 9)\t0.22346730896573733\n",
      "  (0, 19)\t0.22346730896573733\n",
      "  (0, 13)\t0.22346730896573733\n",
      "  (1, 0)\t0.23076792961123066\n",
      "  (1, 17)\t0.32433627313894553\n",
      "  (1, 15)\t0.32433627313894553\n",
      "  (1, 3)\t0.32433627313894553\n",
      "  (1, 14)\t0.32433627313894553\n",
      "  (1, 21)\t0.32433627313894553\n",
      "  (1, 8)\t0.32433627313894553\n",
      "  (1, 12)\t0.32433627313894553\n",
      "  (1, 1)\t0.32433627313894553\n",
      "  (1, 10)\t0.32433627313894553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(df_comments['preprocess'])\n",
    "y_train = df_comments['label']\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "idf_values = vectorizer.idf_\n",
    "\n",
    "print(X_train) # cetak label data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Function to compute raw TF (counts of terms in document)\n",
    "def compute_raw_tf(doc):\n",
    "    words = doc.split()\n",
    "    count = Counter(words)\n",
    "    return count\n",
    "\n",
    "# Compute normalized TF\n",
    "def compute_tf(doc):\n",
    "    words = doc.split()\n",
    "    count = Counter(words)\n",
    "    total_terms = len(words)\n",
    "    tf = {term: count[term] / total_terms for term in count}\n",
    "    return tf\n",
    "\n",
    "# Create DataFrame for IDF\n",
    "idf_df = pd.DataFrame(idf_values, index=terms, columns=[\"IDF\"])\n",
    "\n",
    "# Compute raw TF for each document (term count)\n",
    "raw_tf_dicts = [compute_raw_tf(doc) for doc in df_comments['preprocess']]\n",
    "raw_tf_df = pd.DataFrame(raw_tf_dicts, index=[f'D{i+1}' for i in range(len(df_comments['preprocess']))]).T\n",
    "\n",
    "# Compute normalized TF for each document\n",
    "tf_dicts = [compute_tf(doc) for doc in df_comments['preprocess']]\n",
    "tf_df = pd.DataFrame(tf_dicts, index=[f'D{i+1}' for i in range(len(df_comments['preprocess']))]).T\n",
    "\n",
    "# Fill NaN values with 0\n",
    "raw_tf_df = raw_tf_df.fillna(0)\n",
    "tf_df = tf_df.fillna(0)\n",
    "idf_df = idf_df.fillna(0)\n",
    "\n",
    "\n",
    "# Compute Document Frequency (DF) - number of documents where the term appears\n",
    "df_values = (raw_tf_df > 0).sum(axis=1)\n",
    "\n",
    "# Create final DataFrame\n",
    "final_df = pd.DataFrame(index=terms)\n",
    "final_df['Terms'] = terms\n",
    "\n",
    "# Add raw TF for each document (Raw Terms per document)\n",
    "final_df = final_df.join(raw_tf_df.add_prefix('TF'))  # Add raw term counts for each document\n",
    "\n",
    "# Add normalized TF for each document\n",
    "final_df = final_df.join(tf_df.add_prefix('TFN'))  # Add normalized TF for each document\n",
    "\n",
    "# Add Document Frequency (DF) column\n",
    "final_df['DF'] = df_values\n",
    "\n",
    "# Add IDF column\n",
    "final_df['IDF'] = idf_df['IDF']\n",
    "\n",
    "# Calculate manual TF-IDF by multiplying normalized TF with IDF for each document\n",
    "for doc in [f'D{i+1}' for i in range(len(df_comments['preprocess']))]:\n",
    "    final_df[f'TFIDF_{doc}'] = final_df[f'TFN{doc}'] * final_df['IDF']\n",
    "\n",
    "# Round all numeric columns to 3 decimal places\n",
    "final_df = final_df.round(3)\n",
    "\n",
    "# Export the final DataFrame to CSV\n",
    "final_df.to_csv('train_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latih Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 23 stored elements and shape (2, 22)>\n",
      "  Coords\tValues\n",
      "  (0, 7)\t0.2412282461053091\n",
      "  (0, 11)\t0.40843492476462473\n",
      "  (0, 20)\t0.40843492476462473\n",
      "  (0, 4)\t0.2412282461053091\n",
      "  (0, 6)\t0.2412282461053091\n",
      "  (0, 0)\t0.290604812889593\n",
      "  (0, 16)\t0.2412282461053091\n",
      "  (0, 2)\t0.2412282461053091\n",
      "  (0, 18)\t0.2412282461053091\n",
      "  (0, 5)\t0.2412282461053091\n",
      "  (0, 9)\t0.2412282461053091\n",
      "  (0, 19)\t0.2412282461053091\n",
      "  (0, 13)\t0.2412282461053091\n",
      "  (1, 0)\t0.23076792961123066\n",
      "  (1, 17)\t0.32433627313894553\n",
      "  (1, 15)\t0.32433627313894553\n",
      "  (1, 3)\t0.32433627313894553\n",
      "  (1, 14)\t0.32433627313894553\n",
      "  (1, 21)\t0.32433627313894553\n",
      "  (1, 8)\t0.32433627313894553\n",
      "  (1, 12)\t0.32433627313894553\n",
      "  (1, 1)\t0.32433627313894553\n",
      "  (1, 10)\t0.32433627313894553\n",
      "['aku' 'bagi' 'banyak' 'bau' 'beauty' 'beli' 'blogger' 'cuma' 'gatel'\n",
      " 'hasil' 'hidung' 'ka' 'kulit' 'luvv' 'nyengat' 'pake' 'percaya' 'pernah'\n",
      " 'produk' 'review' 'tasya' 'trus']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:\\\\000-Python-Project\\\\ran-svm\\\\notebook\\\\trained_model.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "result_path = result_path = os.getcwd()\n",
    "model_file = os.path.join(result_path, 'trained_model.pkl')\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    os.remove(model_file)\n",
    "\n",
    "print(X_train)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "model = SVC(random_state=0, kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "joblib.dump(model, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediksi Kelas Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comment = 'gatel' # text train\n",
    "X_test = preprocess_text(new_comment)\n",
    "\n",
    "X_test = vectorizer.transform([X_test])\n",
    "predict = model.predict(X_test)\n",
    "print(predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
