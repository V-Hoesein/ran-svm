{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.48\n",
      "Precision =  0.48\n",
      "Recall =  1.0\n",
      "F1-Score =  0.6486486486486487\n",
      "ConfusionMatrix =  {'TP': np.int64(144), 'TN': np.int64(0), 'FP': np.int64(156), 'FN': np.int64(0)}\n",
      "tp : 144\n",
      "fn : 0\n",
      "sentiment test : [-1 -1  1 -1 -1 -1  1  1  1 -1  1 -1 -1  1 -1  1 -1  1  1  1  1  1  1  1\n",
      " -1 -1  1  1 -1  1 -1 -1 -1 -1  1 -1 -1  1 -1  1  1 -1 -1 -1  1 -1 -1  1\n",
      "  1 -1  1  1  1  1 -1 -1  1  1 -1  1 -1 -1  1  1  1  1  1 -1  1 -1 -1  1\n",
      " -1  1 -1  1  1 -1 -1 -1  1  1  1 -1 -1 -1  1 -1  1 -1  1  1 -1  1 -1  1\n",
      " -1  1  1 -1 -1 -1  1 -1  1 -1 -1 -1 -1  1  1 -1  1 -1  1  1 -1  1  1 -1\n",
      " -1 -1  1 -1  1 -1  1  1 -1 -1  1 -1 -1 -1 -1 -1  1  1 -1 -1  1 -1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1 -1 -1  1  1 -1  1 -1 -1 -1 -1 -1  1  1 -1\n",
      "  1  1 -1 -1  1 -1  1 -1  1 -1 -1  1 -1 -1  1  1 -1 -1 -1  1 -1  1 -1 -1\n",
      "  1  1 -1 -1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1  1 -1  1 -1 -1 -1\n",
      "  1  1  1 -1 -1 -1 -1  1  1  1 -1 -1  1  1 -1  1 -1 -1  1  1 -1  1 -1  1\n",
      " -1  1  1  1 -1  1  1  1  1 -1  1 -1  1 -1  1 -1 -1  1 -1  1 -1 -1  1  1\n",
      " -1 -1 -1  1  1  1 -1 -1  1  1 -1 -1 -1 -1  1  1 -1  1  1 -1 -1  1 -1 -1\n",
      "  1  1 -1  1 -1 -1 -1  1 -1 -1  1  1]\n",
      "sentiment pred : [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = os.path.realpath(os.path.join(os.path.dirname(__name__), '..', 'app','static','uploads', 'dataset.csv'))\n",
    "result_path = os.path.realpath(os.path.join(os.path.dirname(__name__), '..', 'notebook', 'result'))\n",
    "\n",
    "class TFIDFProcessor:\n",
    "    def __init__(self):\n",
    "        # Initialize Sastrawi tools\n",
    "        self.stemmer = StemmerFactory().create_stemmer()\n",
    "        stopword_factory = StopWordRemoverFactory()\n",
    "        self.combined_stopwords = set(stopword_factory.get_stop_words()).union(set(stopwords.words('english')))\n",
    "        self.terms = None  # To store terms after processing corpus\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = self.clean_text(text).lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in self.combined_stopwords]\n",
    "        stemmed = [self.stemmer.stem(word) for word in tokens]\n",
    "        return ' '.join(stemmed)\n",
    "\n",
    "    def compute_raw_tf(self, doc):\n",
    "        words = doc.split()\n",
    "        count = Counter(words)\n",
    "        return count\n",
    "    \n",
    "    def compute_tf(self, doc):\n",
    "        words = doc.split()\n",
    "        count = Counter(words)\n",
    "        total_terms = len(words)\n",
    "        tf = {term: float(count[term]) / total_terms for term in count}\n",
    "        return tf\n",
    "\n",
    "    def compute_idf(self, corpus):\n",
    "        N = len(corpus)\n",
    "        idf_dict = {}\n",
    "        all_words = set(word for doc in corpus for word in doc.split())\n",
    "        \n",
    "        for word in all_words:\n",
    "            containing_docs = sum(1 for doc in corpus if word in doc.split())\n",
    "            idf_dict[word] = float(np.log((1+N) / (1 + containing_docs)) + 1)  # Make sure IDF is a float\n",
    "        \n",
    "        return idf_dict\n",
    "\n",
    "    def compute_tfidf(self, tf_dict, idf_dict):\n",
    "        tfidf_dict = {}\n",
    "        for word, tf_value in tf_dict.items():\n",
    "            tfidf_dict[word] = float(tf_value) * float(idf_dict.get(word, 0.0))  # Ensure both are floats\n",
    "        \n",
    "        return tfidf_dict\n",
    "\n",
    "    def process_corpus(self, corpus):\n",
    "        # Compute IDF for the corpus\n",
    "        idf_values = self.compute_idf(corpus)\n",
    "        \n",
    "        # Get the terms (features)\n",
    "        self.terms = sorted(idf_values.keys())\n",
    "        \n",
    "        # Compute raw TF, normalized TF, and TF-IDF for each document\n",
    "        raw_tf_dicts = [self.compute_raw_tf(doc) for doc in corpus]\n",
    "        tf_dicts = [self.compute_tf(doc) for doc in corpus]\n",
    "        \n",
    "        # Ensure each document's TF-IDF vector contains all terms\n",
    "        tfidf_dicts = []\n",
    "        for tf_dict in tf_dicts:\n",
    "            tfidf_dict = {}\n",
    "            for term in self.terms:\n",
    "                tfidf_dict[term] = tf_dict.get(term, 0) * idf_values.get(term, 0)\n",
    "            tfidf_dicts.append(tfidf_dict)\n",
    "        \n",
    "        # Convert dictionaries to DataFrames for easy manipulation and export\n",
    "        raw_tf_df = pd.DataFrame(raw_tf_dicts, index=[f'D{i+1}' for i in range(len(corpus))]).T\n",
    "        tf_df = pd.DataFrame(tf_dicts, index=[f'D{i+1}' for i in range(len(corpus))]).T\n",
    "        tfidf_df = pd.DataFrame(tfidf_dicts, index=[f'D{i+1}' for i in range(len(corpus))]).T\n",
    "        \n",
    "        # Fill NaN values with 0\n",
    "        raw_tf_df = raw_tf_df.fillna(0)\n",
    "        tf_df = tf_df.fillna(0)\n",
    "        tfidf_df = tfidf_df.fillna(0)\n",
    "        \n",
    "        return self.terms, raw_tf_df, tf_df, tfidf_df, idf_values\n",
    "\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def train_svm(self, X_train, y_train, lr=0.0001, epochs=1000, C=1.0):\n",
    "        num_samples, num_features = X_train.shape\n",
    "        weights = np.zeros(num_features)\n",
    "        bias = 0\n",
    "\n",
    "        # Gradient descent for SVM\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_samples):\n",
    "                condition = y_train[i] * (np.dot(X_train[i], weights) - bias) >= 1\n",
    "                if condition:\n",
    "                    weights -= lr * (2 * C * weights)  # Regularization term\n",
    "                else:\n",
    "                    weights -= lr * (2 * C * weights - np.dot(X_train[i], y_train[i]))\n",
    "                    bias -= lr * y_train[i]\n",
    "        \n",
    "        self.weights, self.bias = weights, bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) - self.bias\n",
    "        return np.sign(linear_output)  # Predict either 1 or -1 based on the sign of the linear output\n",
    "\n",
    "    def compute_decision_values(self, X):\n",
    "        decision_values = np.dot(X, self.weights) - self.bias\n",
    "        return decision_values\n",
    "\n",
    "    def get_weights_bias(self):\n",
    "        return self.weights, self.bias\n",
    "\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(self, dataset_path, result_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.result_path = result_path\n",
    "        \n",
    "        # Initialize processors\n",
    "        self.tfidf_processor = TFIDFProcessor()\n",
    "        self.svm_classifier = SVMClassifier()\n",
    "        \n",
    "        # Load dataset\n",
    "        self.df_comments = pd.read_csv(self.dataset_path)\n",
    "        \n",
    "        # Preprocess text\n",
    "        self.df_comments['preprocess'] = self.df_comments['comment'].apply(self.tfidf_processor.preprocess_text)\n",
    "\n",
    "    def train_model(self):\n",
    "        # Preprocess all text in the dataset\n",
    "        corpus = self.df_comments['preprocess'].tolist()\n",
    "        \n",
    "        # Process corpus for TF-IDF\n",
    "        self.terms, raw_tf_df, tf_df, tfidf_df, idf_values = self.tfidf_processor.process_corpus(corpus)\n",
    "        \n",
    "        # Create Document Frequency (DF)\n",
    "        df_values = (raw_tf_df > 0).sum(axis=1)\n",
    "        \n",
    "        # Create final DataFrame\n",
    "        final_df = pd.DataFrame(index=self.terms)\n",
    "        final_df['Terms'] = self.terms\n",
    "        final_df = final_df.join(raw_tf_df.add_prefix('TF'))  # Add raw term counts for each document\n",
    "        final_df = final_df.join(tf_df.add_prefix('TFN'))  # Add normalized TF for each document\n",
    "        final_df = final_df.join(tfidf_df.add_prefix('TFIDF'))  # Add TF-IDF for each document\n",
    "        \n",
    "        # Add Document Frequency (DF) and IDF values\n",
    "        final_df['DF'] = df_values\n",
    "        final_df['IDF'] = [idf_values.get(term, 0) for term in self.terms]\n",
    "        \n",
    "        # Round all numeric columns to 3 decimal places\n",
    "        final_df = final_df.round(3)\n",
    "        \n",
    "        # Export final DataFrame to CSV\n",
    "        final_df.to_csv(f'{self.result_path}/train_metrics.csv', index=False)\n",
    "        \n",
    "        # Prepare training data for SVM\n",
    "        X = np.array([list(tfidf_df.loc[:, f'D{i+1}']) for i in range(len(corpus))])\n",
    "        y = self.df_comments['label'].values\n",
    "        \n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "        \n",
    "        # Train SVM manually\n",
    "        self.svm_classifier.train_svm(X_train, y_train)\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = self.svm_classifier.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.calculate_metrics(y_test, y_pred)\n",
    "        \n",
    "        # Export metrics to CSV\n",
    "        metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "        metrics_df.to_csv(f'{self.result_path}/metrics.csv', index=False)\n",
    "        \n",
    "        # Export decision values to CSV\n",
    "        decision_values = self.svm_classifier.compute_decision_values(X_test)\n",
    "        decision_values_df = pd.DataFrame(decision_values, index=[f'D{i+1}' for i in range(len(X_test))], columns=['DecisionValue'])\n",
    "        decision_values_df.to_csv(f'{self.result_path}/decision_values.csv', index=True)\n",
    "\n",
    "        # Export weights and bias to CSV\n",
    "        weights, bias = self.svm_classifier.get_weights_bias()\n",
    "        weights_bias_df = pd.DataFrame({'Weights': weights, 'Bias': [bias] * len(weights)}, index=[f'Feature{i+1}' for i in range(len(weights))])\n",
    "        weights_bias_df.to_csv(f'{self.result_path}/weights_bias.csv', index=True)\n",
    "    \n",
    "        print(f'sentiment test : {y_test}')\n",
    "        print(f'sentiment pred : {y_pred}')\n",
    "        \n",
    "        def calculate_metrics(self, y_true, y_pred):\n",
    "        # Calculate confusion matrix\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        tn = np.sum((y_true == -1) & (y_pred == -1))\n",
    "        fp = np.sum((y_true == -1) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == -1))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_samples = tp + tn + fp + fn\n",
    "        accuracy = (tp + tn) / total_samples if total_samples > 0 else 0\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'F1-Score: {f1_score}')\n",
    "        print(f'ConfusionMatrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}')\n",
    "        \n",
    "        return {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1_score,\n",
    "            'ConfusionMatrix': {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize TextClassifier with paths\n",
    "text_classifier = TextClassifier(dataset_path, result_path)\n",
    "\n",
    "# Train the model and save results\n",
    "text_classifier.train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
